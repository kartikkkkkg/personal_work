import pandas as pd
import msoffcrypto
import io
import re
import os
from tqdm.autonotebook import tqdm
from openpyxl import load_workbook
from openpyxl.worksheet.datavalidation import DataValidation
from openpyxl.utils import get_column_letter

# Register tqdm with pandas to show progress bars
tqdm.pandas(desc="Processing data")

# --- Configuration for all files ---
file_path = "input.xlsx"
password = "your_password"
new_data_filename = "ProjectID_clarity.xlsx"
mapping_filename = "mapping.xlsx"
final_output_filename = "final_report.xlsx"

# --- Load password-protected Excel (Steps 1, 2, 3) ---
try:
    with open(file_path, "rb") as f:
        decrypted = io.BytesIO()
        office_file = msoffcrypto.OfficeFile(f)
        office_file.load_key(password=password)
        office_file.decrypt(decrypted)
        
    df = pd.read_excel(decrypted)
    df.columns = df.columns.str.strip()
    print("Successfully read and cleaned column names for the Excel file!")

except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    print("Please check your file path and password.")
    exit()

# --- Step 1 & 2: Filtering and Merging (Omitting function definitions for space) ---
df["Status"] = df["Status"].fillna('').astype(str).str.strip().str.lower()
df["Latest Approver Name"] = df["Latest Approver Name"].fillna('').astype(str).str.strip()
df = df[(df["Status"] == "pending") & (df["Latest Approver Name"].str.contains("Aini Ayoub|Sybi", case=False, na=False))]

def merge_and_extract(group):
    merged = group.iloc[0].copy()
    merged_comments = ";".join(group["Comments"].dropna().astype(str).unique())
    merged["Comments"] = merged_comments
    cw_numbers = re.findall(r'[Cc][Ww]\d{6}|\b\d{6}\b', merged_comments)
    bank_ids = re.findall(r'\b\d{7}\b', merged_comments)
    project_ids = re.findall(r'\b[12]\d{7}\b', merged_comments)
    merged["CW_Number"] = ";".join(pd.Series(cw_numbers).unique()) if cw_numbers else None
    merged["Bank_ID"] = ";".join(pd.Series(bank_ids).unique()) if bank_ids else None
    merged["Project_ID"] = ";".join(pd.Series(project_ids).unique()) if project_ids else None
    return merged
df_final = df.groupby("Position Code").progress_apply(merge_and_extract).reset_index(drop=True)

# ... (Steps 3, 4, 5, 6 - Joins and Lookups) ...
try:
    df_new_sheets = pd.read_excel(new_data_filename, sheet_name=None)
    df_project_clarity = df_new_sheets['ProjectID_clarity']
    df_critical = df_new_sheets['Critical']
    df_project_clarity.columns = df_project_clarity.columns.str.strip()
    df_critical.columns = df_critical.columns.str.strip()
except Exception as e:
    df_project_clarity = pd.DataFrame()
    df_critical = pd.DataFrame()
try:
    df_mapping = pd.read_excel(mapping_filename, sheet_name='mapping')
    df_mapping.columns = df_mapping.columns.str.strip()
except Exception as e:
    df_mapping = pd.DataFrame()
    
# Placeholder logic for Joins (Needed to define final columns)
if not df_mapping.empty:
    df_mapping['Country'] = df_mapping['Country'].astype(str).str.strip().str.lower()
    df_mapping['Business Function Lvl6 Name'] = df_mapping['Business Function Lvl6 Name'].astype(str).str.strip().str.lower()
    cost_df_unique = df_mapping.drop_duplicates(subset=['Country'], keep='first'); cost_map = cost_df_unique.set_index('Country')['high/low'].to_dict()
    df_final['Country_Key'] = df_final['Country'].astype(str).str.strip().str.lower(); HIGH_COST_COUNTRIES = ['singapore', 'hong kong', 'united arab emirates', 'united states', 'united kingdom']
    df_final['Cost'] = df_final['Country_Key'].apply(lambda x: 'high' if x in HIGH_COST_COUNTRIES else 'low'); df_final.drop(columns=['Country_Key'], inplace=True)
    domain_df_unique = df_mapping.drop_duplicates(subset=['Business Function Lvl6 Name'], keep='first')
    domain_map = domain_df_unique.set_index('Business Function Lvl6 Name')[['MT Domain', 'MT-1 Domain', 'Cost Cat']].to_dict('index')
    df_final['Business Function L6_Key'] = df_final['Business Function L6'].astype(str).str.strip().str.lower()
    def domain_lookup(bf_l6): return domain_map.get(bf_l6, {'MT Domain': None, 'MT-1 Domain': None, 'Cost Cat': None})
    domain_results = df_final['Business Function L6_Key'].apply(domain_lookup)
    df_final['MT Domain'] = [r['MT Domain'] for r in domain_results]; df_final['MT-1 Domain'] = [r['MT-1 Domain'] for r in domain_results]
    df_final['Cost Category'] = [r['Cost Cat'] for r in domain_results]; df_final.drop(columns=['Business Function L6_Key'], inplace=True)
else: df_final[['Cost', 'MT Domain', 'MT-1 Domain', 'Cost Category']] = None, None, None, None
if not df_critical.empty:
    df_critical['Job Family'] = df_critical['Job Family'].astype(str).str.strip().str.lower()
    critical_df_unique = df_critical.drop_duplicates(subset=['Job Family'], keep='first')
    critical_jf_map = critical_df_unique.set_index('Job Family')['Critical JF'].to_dict()
    df_final['Job Family_Key'] = df_final['Job Family'].astype(str).str.strip().str.lower()
    df_final['Critical JF'] = df_final['Job Family_Key'].apply(lambda x: critical_jf_map.get(x, 'Others'))
    df_final.drop(columns=['Job Family_Key'], inplace=True)
else: df_final['Critical JF'] = 'Others'
if not df_project_clarity.empty:
    df_project_clarity['Project ID'] = df_project_clarity['Project ID'].astype(str).str.strip()
    project_clarity_map = df_project_clarity.set_index('Project ID')
    qpr_col = next((col for col in df_project_clarity.columns if 'QPR' in col.upper()), None)
    lifecycle_col = 'Lifecycle status' 
    if qpr_col:
        def lookup_and_join(project_ids_str): 
            if pd.isna(project_ids_str) or not isinstance(project_ids_str, str): return None, None, None
            ids_list = [id.strip() for id in project_ids_str.split(';')]
            project_names, lifecycle_statuses, qpr_values = [], [], []
            for pid in ids_list:
                if pid in project_clarity_map.index:
                    row = project_clarity_map.loc[pid]
                    project_names.append(row['Project Name']); lifecycle_statuses.append(row[lifecycle_col]); qpr_values.append(row[qpr_col])
            return (";".join(pd.Series(project_names).dropna().unique()) if project_names else None,
                    ";".join(pd.Series(lifecycle_statuses).dropna().unique()) if lifecycle_statuses else None,
                    ";".join(pd.Series(qpr_values).dropna().unique()) if qpr_values else None)
        df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = df_final['Project_ID'].progress_apply(
            lambda x: pd.Series(lookup_and_join(x)))
    else: df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = None, None, None
else: df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = None, None, None


# --- Step 7: Add New Review/Dropdown Columns and Final Selection/Ordering ---

# DEFINE THE 10 REVIEW COLUMNS
REVIEW_COLUMNS = [
    "Spans (of direct reports)", "T&O MT Review", "HRBP Review", "Have you considered an Apprentice for this role?", 
    "Why not low Cost Location?", "Niche Skillset", "TA Confirmed Skillset not available in Hubs", 
    "If Reg Requirement pls state which Reg", "Critical Replacement on Fin Outlook", "3 month data volume trend"
]

# ADD 10 EMPTY COLUMNS TO DF
for col in REVIEW_COLUMNS:
    df_final[col] = None

# DEFINE DROPDOWN MAPPING (Output Column Name -> Dropdown Sheet Column Index)
DROPDOWN_MAPPING = {
    "T&O MT Review": 0, "HRBP Review": 1, "Have you considered an Apprentice for this role?": 2, 
    "Why not low Cost Location?": 3, "Niche Skillset": 4, "TA Confirmed Skillset not available in Hubs": 5, 
    "If Reg Requirement pls state which Reg": 6, "Critical Replacement on Fin Outlook": 7, "3 month data volume trend": 8
}

selected_columns = [
    "Bank ID of requestor", "Business Function", "MT Domain", "MT-1 Domain", "Cost Category", "Business Function L6",
    "Position Code", "Position Title", "Job Family", "Critical JF", "Country", "Location (Location Name)", "Cost",
    "Employment Type", "Global Grade", "Comments", "Reason for Hire", "People Leader Position", 
    "CW_Number", "Bank_ID", "Project_ID", "Project Name", "Lifecycle Status", "Quarterly Performance Review(QPR)", 
    *REVIEW_COLUMNS
]

df_final = df_final[selected_columns]
df_final.sort_values(by=['MT Domain', 'MT-1 Domain', 'Cost Category'], inplace=True, na_position='last')


# --- Step 8: Save to new Excel, ADDING DROPDOWNS (ROBUST LINKED LISTS) ---

HIDDEN_SHEET_NAME = '_DropdownLists'

# 8a: Load Dropdown Values from mapping.xlsx
try:
    df_dropdown = pd.read_excel(mapping_filename, sheet_name='drop-down')
except Exception as e:
    # (Fallback Save - omitted for brevity)
    exit()

# 8b: Save DataFrame to a Temporary File
temp_output_filename = "temp_output.xlsx"
df_final.to_excel(temp_output_filename, index=False, engine='openpyxl')


# 8c: Apply Data Validation using openpyxl
try:
    wb = load_workbook(temp_output_filename)
    ws = wb.active
    
    # 1. Create the hidden sheet and write the lists
    ws_hidden = wb.create_sheet(HIDDEN_SHEET_NAME)
    ws_hidden.sheet_state = 'hidden' # Hides the sheet from the user
    
    # Write the dropdown data from the loaded DataFrame to the hidden sheet
    for list_index, col_name in DROPDOWN_MAPPING.items():
        list_data = df_dropdown.iloc[:, col_name].dropna().unique()
        
        # Write each option down the column (starting at A1)
        for row_index, option in enumerate(list_data):
            ws_hidden.cell(row=row_index + 1, column=list_index + 1, value=str(option))
            
        # 2. Define the range for the dropdown rule (e.g., =_HiddenLists!$A$1:$A$10)
        # Get the max row written for this list
        max_row = len(list_data)
        col_letter = get_column_letter(list_index + 1)
        
        # Create the formula to link to the hidden range
        formula_range = f'={HIDDEN_SHEET_NAME}!${col_letter}$1:${col_letter}${max_row}'
        
        # 3. Apply the Data Validation to the corresponding output column
        output_col_index_in_list = selected_columns.index(col_name)
        excel_col_letter = get_column_letter(output_col_index_in_list + 1)
        
        # Apply from Row 2 (first data row) down to the last row of data
        range_string = f'{excel_col_letter}2:{excel_col_letter}{ws.max_row}'
        
        data_validation = DataValidation(type="list", formula1=formula_range, allow_blank=True)
        data_validation.add(range_string)
        ws.add_data_validation(data_validation)
        
    # 8d: Save the final file, handling the overwriting avoidance
    base_filename = final_output_filename
    counter = 1
    final_save_filename = base_filename

    while os.path.exists(final_save_filename):
        name, ext = os.path.splitext(base_filename)
        final_save_filename = f"{name}_{counter}{ext}"
        counter += 1
        
    wb.save(final_save_filename)
    os.remove(temp_output_filename) # Clean up temp file

    print(f"\n🎉 All processing complete. The final report with robust dropdowns has been saved to '{final_save_filename}'.")

except Exception as e:
    print(f"\nFATAL ERROR during Data Validation: {e}")
    print("The final file was saved without dropdowns. Please ensure openpyxl is installed and your 'drop-down' sheet exists.")
