import pandas as pd
import msoffcrypto
import io
import re
import os
from tqdm.autonotebook import tqdm
from openpyxl import load_workbook
from openpyxl.worksheet.datavalidation import DataValidation

# Register tqdm with pandas to show progress bars
tqdm.pandas(desc="Processing data")

# --- Configuration for all files ---
file_path = "input.xlsx"
password = "your_password"
new_data_filename = "ProjectID_clarity.xlsx"
mapping_filename = "mapping.xlsx"
final_output_filename = "final_report.xlsx"

# --- Load password-protected Excel ---
try:
    with open(file_path, "rb") as f:
        decrypted = io.BytesIO()
        office_file = msoffcrypto.OfficeFile(f)
        office_file.load_key(password=password)
        office_file.decrypt(decrypted)
        
    df = pd.read_excel(decrypted)
    df.columns = df.columns.str.strip()
    print("Successfully read and cleaned column names for the Excel file!")

except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found.")
    exit()
except Exception as e:
    print(f"An error occurred: {e}")
    print("Please check your file path and password.")
    exit()

# --- Step 1: Filter rows first based on your specific criteria ---
df["Status"] = df["Status"].fillna('').astype(str).str.strip().str.lower()
df["Latest Approver Name"] = df["Latest Approver Name"].fillna('').astype(str).str.strip()

df = df[(df["Status"] == "pending") & (df["Latest Approver Name"].str.contains("Aini Ayoub|Sybi", case=False, na=False))]

print(f"\nAfter filtering, there are {len(df)} rows remaining.")
print(f"After filtering, there are {df['Position Code'].nunique()} unique Position IDs.")


# --- Step 2: Merge rows based on Position Code and extract IDs from merged comments ---
def merge_and_extract(group):
    merged = group.iloc[0].copy()
    merged_comments = ";".join(group["Comments"].dropna().astype(str).unique())
    merged["Comments"] = merged_comments
    
    cw_numbers = re.findall(r'[Cc][Ww]\d{6}|\b\d{6}\b', merged_comments)
    bank_ids = re.findall(r'\b\d{7}\b', merged_comments)
    project_ids = re.findall(r'\b[12]\d{7}\b', merged_comments)
    
    merged["CW_Number"] = ";".join(pd.Series(cw_numbers).unique()) if cw_numbers else None
    merged["Bank_ID"] = ";".join(pd.Series(bank_ids).unique()) if bank_ids else None
    merged["Project_ID"] = ";".join(pd.Series(project_ids).unique()) if project_ids else None
    
    return merged

df_final = df.groupby("Position Code").progress_apply(merge_and_extract).reset_index(drop=True)


# --- Step 3: Load the new data files for all subsequent joins ---
try:
    df_new_sheets = pd.read_excel(new_data_filename, sheet_name=None)
    df_project_clarity = df_new_sheets['ProjectID_clarity']
    df_critical = df_new_sheets['Critical']
    df_project_clarity.columns = df_project_clarity.columns.str.strip()
    df_critical.columns = df_critical.columns.str.strip()
    print(f"\nSuccessfully loaded '{new_data_filename}' for joins.")
except Exception as e:
    print(f"Error loading '{new_data_filename}'. Skipping joins: {e}")
    df_project_clarity = pd.DataFrame()
    df_critical = pd.DataFrame()

# Load mapping.xlsx
try:
    df_mapping = pd.read_excel(mapping_filename, sheet_name='mapping')
    df_mapping.columns = df_mapping.columns.str.strip()
    print(f"Successfully loaded '{mapping_filename}' for cost and domain joins.")
except Exception as e:
    print(f"Error loading '{mapping_filename}'. Skipping cost/domain joins: {e}")
    df_mapping = pd.DataFrame()


# --- Step 4: Cost & Domain Joins from 'mapping' sheet (WITH DUPLICATE RESOLUTION) ---
if not df_mapping.empty:
    
    # NORMALIZE KEYS in Lookup Table
    df_mapping['Country'] = df_mapping['Country'].astype(str).str.strip().str.lower()
    df_mapping['Business Function Lvl6 Name'] = df_mapping['Business Function Lvl6 Name'].astype(str).str.strip().str.lower()
    
    # 4a: Cost Mapping (Country -> Cost)
    HIGH_COST_COUNTRIES = ['singapore', 'hong kong', 'united arab emirates', 'united states', 'united kingdom']
    
    df_final['Country_Key'] = df_final['Country'].astype(str).str.strip().str.lower()
    
    df_final['Cost'] = df_final['Country_Key'].apply(lambda x: 'high' if x in HIGH_COST_COUNTRIES else 'low')
    df_final.drop(columns=['Country_Key'], inplace=True)
    print("✅ Joined 'Cost' data based on explicit country list.")

    # 4b: Domain Mapping (Business Function L6 -> MT Domain, MT-1 Domain, Cost Cat)
    domain_df_unique = df_mapping.drop_duplicates(subset=['Business Function Lvl6 Name'], keep='first')
    domain_map = domain_df_unique.set_index('Business Function Lvl6 Name')[['MT Domain', 'MT-1 Domain', 'Cost Cat']].to_dict('index')
    
    df_final['Business Function L6_Key'] = df_final['Business Function L6'].astype(str).str.strip().str.lower()
    
    def domain_lookup(bf_l6):
        return domain_map.get(bf_l6, {'MT Domain': None, 'MT-1 Domain': None, 'Cost Cat': None})

    domain_results = df_final['Business Function L6_Key'].apply(domain_lookup)
    df_final['MT Domain'] = [r['MT Domain'] for r in domain_results]
    df_final['MT-1 Domain'] = [r['MT-1 Domain'] for r in domain_results]
    df_final['Cost Category'] = [r['Cost Cat'] for r in domain_results]
    df_final.drop(columns=['Business Function L6_Key'], inplace=True)
    print("✅ Joined 'MT Domain', 'MT-1 Domain', and 'Cost Category' data.")

else:
    print("⚠️ Skipping cost and domain joins as the mapping file was not found.")
    df_final[['Cost', 'MT Domain', 'MT-1 Domain', 'Cost Category']] = None, None, None, None


# --- Step 5: Join from the 'Critical' sheet based on Job family (WITH DUPLICATE RESOLUTION) ---
if not df_critical.empty:
    df_critical['Job Family'] = df_critical['Job Family'].astype(str).str.strip().str.lower()
    critical_df_unique = df_critical.drop_duplicates(subset=['Job Family'], keep='first')
    critical_jf_map = critical_df_unique.set_index('Job Family')['Critical JF'].to_dict()

    df_final['Job Family_Key'] = df_final['Job Family'].astype(str).str.strip().str.lower()
    df_final['Critical JF'] = df_final['Job Family_Key'].apply(lambda x: critical_jf_map.get(x, 'Others'))
    df_final.drop(columns=['Job Family_Key'], inplace=True)
    print("✅ Joined 'Critical JF' data.")
else:
    df_final['Critical JF'] = 'Others'


# --- Step 6: Join from the 'ProjectID_clarity' sheet based on Project ID ---
if not df_project_clarity.empty:
    df_project_clarity['Project ID'] = df_project_clarity['Project ID'].astype(str).str.strip()
    project_clarity_map = df_project_clarity.set_index('Project ID')

    qpr_col = next((col for col in df_project_clarity.columns if 'QPR' in col.upper()), None)
    lifecycle_col = 'Lifecycle status' 
    
    if not qpr_col:
        print("Error: Could not find 'Quarterly Performance Review(QPR)' column in ProjectID_clarity sheet.")
        df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = None, None, None
    else:
        def lookup_and_join(project_ids_str):
            if pd.isna(project_ids_str) or not isinstance(project_ids_str, str):
                return None, None, None
            
            ids_list = [id.strip() for id in project_ids_str.split(';')]
            project_names, lifecycle_statuses, qpr_values = [], [], []
            
            for pid in ids_list:
                if pid in project_clarity_map.index:
                    row = project_clarity_map.loc[pid]
                    project_names.append(row['Project Name'])
                    lifecycle_statuses.append(row[lifecycle_col]) 
                    qpr_values.append(row[qpr_col])
            
            return (
                ";".join(pd.Series(project_names).dropna().unique()) if project_names else None,
                ";".join(pd.Series(lifecycle_statuses).dropna().unique()) if lifecycle_statuses else None,
                ";".join(pd.Series(qpr_values).dropna().unique()) if qpr_values else None
            )

        df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = df_final['Project_ID'].progress_apply(
            lambda x: pd.Series(lookup_and_join(x))
        )
        print("✅ Joined 'ProjectID_clarity' data.")
else:
    print("⚠️ Skipping 'ProjectID_clarity' sheet join.")
    df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = None, None, None


# --- Step 7: Add New Review/Dropdown Columns and Final Selection/Ordering ---

# DEFINE THE 10 REVIEW COLUMNS
REVIEW_COLUMNS = [
    "Spans (of direct reports)", "T&O MT Review", "HRBP Review", "Have you considered an Apprentice for this role?", 
    "Why not low Cost Location?", "Niche Skillset", "TA Confirmed Skillset not available in Hubs", 
    "If Reg Requirement pls state which Reg", "Critical Replacement on Fin Outlook", "3 month data volume trend"
]

# ADD 10 EMPTY COLUMNS TO DF
for col in REVIEW_COLUMNS:
    df_final[col] = None

# DEFINE THE 9 COLUMNS THAT NEED DROPDOWN LISTS (Excluding 'Spans')
DROPDOWN_COLUMNS_WITH_LISTS = [
    "T&O MT Review", "HRBP Review", "Have you considered an Apprentice for this role?", 
    "Why not low Cost Location?", "Niche Skillset", "TA Confirmed Skillset not available in Hubs", 
    "If Reg Requirement pls state which Reg", "Critical Replacement on Fin Outlook", "3 month data volume trend"
]

selected_columns = [
    "Bank ID of requestor", 
    "Business Function",
    "MT Domain",
    "MT-1 Domain",
    "Cost Category",
    "Business Function L6",
    "Position Code",
    "Position Title",
    "Job Family",
    "Critical JF",
    "Country",
    "Location (Location Name)",
    "Cost",
    "Employment Type",
    "Global Grade",
    "Comments",
    "Reason for Hire",
    "People Leader Position", 
    
    "CW_Number",
    "Bank_ID",
    "Project_ID",
    "Project Name", 
    "Lifecycle Status",
    "Quarterly Performance Review(QPR)", 
    
    # NEW REVIEW COLUMNS (Placed at the end)
    *REVIEW_COLUMNS
]

df_final = df_final[selected_columns]

# Sort the DataFrame as requested
df_final.sort_values(
    by=['MT Domain', 'MT-1 Domain', 'Cost Category'], 
    inplace=True, 
    na_position='last'
)
print("✅ Final file sorted by MT Domain, MT-1 Domain, and Cost Category.")


# --- Step 8: Save to new Excel, ADDING DROPDOWNS ---

# 8a: Define the mapping of output column name to its starting list in the dropdown sheet (Col A, B, C...)
# 'T&O MT Review' (Output Column 2) needs the list from Dropdown Sheet Column A (Index 0).
# The rest follow sequentially.
DROPDOWN_MAPPING = {
    "T&O MT Review": 0, # Col A (Index 0) of the dropdown sheet
    "HRBP Review": 1, 
    "Have you considered an Apprentice for this role?": 2, 
    "Why not low Cost Location?": 3, 
    "Niche Skillset": 4, 
    "TA Confirmed Skillset not available in Hubs": 5, 
    "If Reg Requirement pls state which Reg": 6, 
    "Critical Replacement on Fin Outlook": 7, 
    "3 month data volume trend": 8
}

# 8b: Load Dropdown Values from mapping.xlsx
try:
    df_dropdown = pd.read_excel(mapping_filename, sheet_name='drop-down')
    print("Successfully loaded 'drop-down' sheet for validation.")
except Exception as e:
    print(f"Error loading 'drop-down' sheet: {e}. Saving without dropdowns.")
    
    # Fallback Save (Saves without custom formatting/dropdowns)
    base_filename = final_output_filename
    counter = 1
    output_filename = base_filename
    while os.path.exists(output_filename):
        name, ext = os.path.splitext(base_filename)
        output_filename = f"{name}_{counter}{ext}"
        counter += 1
    df_final.to_excel(output_filename, index=False)
    print(f"\nFATAL ERROR: Could not apply dropdowns. Report saved to '{output_filename}' without them.")
    exit()

# 8c: Save DataFrame to a Temporary File
temp_output_filename = "temp_output.xlsx"
df_final.to_excel(temp_output_filename, index=False, engine='openpyxl')


# 8d: Apply Data Validation using openpyxl
try:
    wb = load_workbook(temp_output_filename)
    ws = wb.active
    
    # Find the starting position of the dropdowns in the output file
    output_col_start_index = selected_columns.index("T&O MT Review") + 1 
    
    for col_name, list_index in DROPDOWN_MAPPING.items():
        
        # 1. Determine the EXCEL column letter for the output file
        # The index in the selected_columns list corresponds to the output column number
        output_col_index_in_list = selected_columns.index(col_name)
        excel_col_letter = chr(ord('A') + output_col_index_in_list) 
        
        # 2. Get the list of unique values from the drop-down sheet column
        dropdown_series = df_dropdown.iloc[:, list_index].dropna().unique()
        dropdown_list = [str(val) for val in dropdown_series if str(val).strip()]
        
        if not dropdown_list:
             print(f"Warning: No valid dropdown options found in Column {chr(ord('A') + list_index)} for '{col_name}'.")
             continue
             
        # Create the comma-separated list for Excel Data Validation
        formula_content = ','.join(dropdown_list)
        if len(formula_content) > 250:
             formula_content = ','.join(dropdown_list[:10]) 

        formula = '"{}"'.format(formula_content)
        
        # Define the range to apply the dropdown (from row 2 down to the last row)
        data_validation = DataValidation(type="list", formula1=formula, allow_blank=True)
        ws.add_data_validation(data_validation)
        data_validation.ranges.append(f'{excel_col_letter}2:{excel_col_letter}{ws.max_row}')
        
    # 8e: Save the final file, handling the overwriting avoidance
    base_filename = final_output_filename
    counter = 1
    final_save_filename = base_filename

    while os.path.exists(final_save_filename):
        name, ext = os.path.splitext(base_filename)
        final_save_filename = f"{name}_{counter}{ext}"
        counter += 1
        
    wb.save(final_save_filename)
    os.remove(temp_output_filename) # Clean up temp file

    print(f"\n🎉 All processing complete. The final report with dropdowns has been saved to '{final_save_filename}'.")

except Exception as e:
    print(f"\nFATAL ERROR during Data Validation: {e}")
    print("The final file was saved without dropdowns. Please ensure openpyxl is installed and your 'drop-down' sheet columns contain data.")
