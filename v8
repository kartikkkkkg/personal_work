import pandas as pd
import msoffcrypto
import io
import re
import os
from tqdm.autonotebook import tqdm

tqdm.pandas(desc="Processing data")

# --- Configuration for all files ---
RAW_DATA_FOLDER = "../Raw Data/"
OUTPUT_FOLDER = "../Output Reports/"
file_path = RAW_DATA_FOLDER + "input.xlsx"
password = "your_password"
new_data_filename = RAW_DATA_FOLDER + "ProjectID_clarity.xlsx"
mapping_filename = RAW_DATA_FOLDER + "mapping.xlsx"
final_output_base_filename = "final_report.xlsx"

# --- Load password-protected Excel ---
try:
    with open(file_path, "rb") as f:
        decrypted = io.BytesIO()
        office_file = msoffcrypto.OfficeFile(f)
        office_file.load_key(password=password)
        office_file.decrypt(decrypted)
        
    df = pd.read_excel(decrypted)
    df.columns = df.columns.str.strip()
    print("Successfully read and cleaned column names for the Excel file!")

except Exception as e:
    print(f"Error: {e}. Please check file path, password, and dependencies.")
    df = pd.DataFrame() 
    print("Skipping further execution due to file error.")


if not df.empty:
    # --- Step 1: Filter rows first based on your specific criteria ---
    df["Status"] = df["Status"].fillna('').astype(str).str.strip().str.lower()
    df["Latest Approver Name"] = df["Latest Approver Name"].fillna('').astype(str).str.strip()

    df = df[
        (df["Status"] == "pending") & 
        (df["Latest Approver Name"].str.contains("Aini Ayoub|Sybi", case=False, na=False)) ]

    print(f"\nAfter filtering, there are {len(df)} rows remaining.")
    print(f"After filtering, there are {df['Position Code'].nunique()} unique Position IDs.")


    # --- Step 2: Merge rows based on Position Code and extract IDs from merged comments ---
    def merge_and_extract(group):
        merged = group.iloc[0].copy()
        merged_comments = ";".join(group["Comments"].dropna().astype(str).unique())
        merged["Comments"] = merged_comments
        
        cw_numbers = re.findall(r'[Cc][Ww]\d{6}|\b\d{6}\b', merged_comments)
        bank_ids = re.findall(r'\b\d{7}\b', merged_comments)
        project_ids = re.findall(r'\b[12]\d{7}\b', merged_comments)
        
        merged["CW_Number"] = ";".join(pd.Series(cw_numbers).unique()) if cw_numbers else None
        merged["Bank_ID"] = ";".join(pd.Series(bank_ids).unique()) if bank_ids else None
        merged["Project_ID"] = ";".join(pd.Series(project_ids).unique()) if project_ids else None
        
        return merged

    if not df.empty:
        df_final = df.groupby("Position Code").progress_apply(merge_and_extract).reset_index(drop=True)
    else:
        df_final = pd.DataFrame(columns=df.columns.tolist() + ['CW_Number', 'Bank_ID', 'Project_ID'])

    # --- Step 3: Load the new data files for all subsequent joins ---
    try:
        df_new_sheets = pd.read_excel(new_data_filename, sheet_name=None)
        df_project_clarity = df_new_sheets['ProjectID_clarity']
        df_project_clarity.columns = df_project_clarity.columns.str.strip()
        print(f"\nSuccessfully loaded '{new_data_filename}' for joins.")
    except Exception as e:
        df_project_clarity = pd.DataFrame()

    # Load mapping.xlsx
    try:
        df_mapping = pd.read_excel(mapping_filename, sheet_name='mapping')
        df_mapping.columns = df_mapping.columns.str.strip()
        print(f"Successfully loaded '{mapping_filename}' for cost and domain joins.")
    except Exception as e:
        df_mapping = pd.DataFrame()


    # --- Step 4: Cost & Domain Joins and CONDITIONAL COUNTRY OVERWRITE ---
    if not df_mapping.empty and not df_final.empty:
        
        df_mapping['Country'] = df_mapping['Country'].astype(str).str.strip().str.lower()
        df_mapping['Business Function Lvl6 Name'] = df_mapping['Business Function Lvl6 Name'].astype(str).str.strip().str.lower()
        
        HIGH_COST_COUNTRIES = ['singapore', 'hong kong', 'united arab emirates', 'united states', 'united kingdom']
        
        df_final['Country_Key'] = df_final['Country'].astype(str).str.replace(r'[\xa0\s]+', ' ', regex=True).str.strip().str.lower()
        df_final['Location_Key'] = df_final['Location (Location Name)'].astype(str).str.replace(r'[\xa0\s]+', ' ', regex=True).str.strip().str.lower()
        
        df_final['Cost'] = df_final['Country_Key'].apply(lambda x: 'high' if x in HIGH_COST_COUNTRIES else 'low')
        
        is_singapore = df_final['Country_Key'] == 'singapore'
        
        # ADDED 'tianjin' to the China overwrite
        guangzhou_tianjin_mask = is_singapore & df_final['Location_Key'].str.contains('guangzhou|tianjin', case=False, na=False, regex=True)
        df_final.loc[guangzhou_tianjin_mask, 'Country'] = 'China' 
        df_final.loc[guangzhou_tianjin_mask, 'Cost'] = 'low'
        
        india_mask = is_singapore & df_final['Location_Key'].str.contains('bangalore|chennai', case=False, na=False, regex=True)
        df_final.loc[india_mask, 'Country'] = 'India' 
        df_final.loc[india_mask, 'Cost'] = 'low'
        
        malaysia_mask = is_singapore & df_final['Location_Key'].str.contains('kuala lumpur', case=False, na=False, regex=False)
        df_final.loc[malaysia_mask, 'Country'] = 'Malaysia'
        df_final.loc[malaysia_mask, 'Cost'] = 'low'
        
        df_final['Country'] = df_final['Country'].str.title()
        
        df_final.drop(columns=['Country_Key', 'Location_Key'], inplace=True)

        # RENAMED cost values
        df_final['Cost'].replace({'high': 'H', 'low': 'L'}, inplace=True)
        
        # --- MT Domain and Cost Cat Joins (using Business Function L6) ---
        domain_df_unique = df_mapping.drop_duplicates(subset=['Business Function Lvl6 Name'], keep='first')
        domain_map = domain_df_unique.set_index('Business Function Lvl6 Name')[['MT Domain', 'MT-1 Domain', 'Cost Cat']].to_dict('index')
        
        df_final['Business Function L6_Key'] = df_final['Business Function L6'].astype(str).str.strip().str.lower()
        def domain_lookup(bf_l6): return domain_map.get(bf_l6, {'MT Domain': None, 'MT-1 Domain': None, 'Cost Cat': None})
        domain_results = df_final['Business Function L6_Key'].apply(domain_lookup)
        df_final['MT Domain'] = [r['MT Domain'] for r in domain_results]
        df_final['MT-1 Domain'] = [r['MT-1 Domain'] for r in domain_results]
        df_final['Cost Category'] = [r['Cost Cat'] for r in domain_results]
        df_final.drop(columns=['Business Function L6_Key'], inplace=True)
        print("✅ Joined 'MT Domain', 'MT-1 Domain', and 'Cost Category' data (from BFL6).")

        # --- Financial Outlook Mapping (using the separate MT Domain column Q) ---
        
        # 1. Determine the column name for the second MT Domain (Column Q = Index 16)
        try:
            if len(df_mapping.columns) > 16:
                mt_domain_q_col_name = df_mapping.columns[16]
            else:
                # If 'MT Domain' column from BFL6 is the key, this is a fallback
                mt_domain_q_col_name = 'MT Domain' 

            # 2. Clean both the lookup key (MT Domain from Q) and the join key (MT Domain in df_final)
            df_mapping[mt_domain_q_col_name] = df_mapping[mt_domain_q_col_name].astype(str).str.strip()
            df_final['MT Domain (Q_Key)'] = df_final['MT Domain'].astype(str).str.strip() 
            
            # 3. Create the outlook map using the specific MT Domain (Q) as the key
            outlook_map = df_mapping.drop_duplicates(subset=[mt_domain_q_col_name], keep='first').set_index(mt_domain_q_col_name)['Financial Outlook'].to_dict()
            
            # 4. Map the data
            df_final['Financial outlook'] = df_final['MT Domain (Q_Key)'].map(outlook_map)
            df_final.drop(columns=['MT Domain (Q_Key)'], inplace=True)
            print(f"✅ Joined 'Financial outlook' data using MT Domain column '{mt_domain_q_col_name}' (Q) as the key.")
            
        except (KeyError, IndexError) as e:
            print(f"⚠️ ERROR: Could not map Financial Outlook. Reason: {e}. Setting 'Financial outlook' to None.")
            df_final['Financial outlook'] = None

    # --- Step 5: Roll Up Name Join (NEW STEP) ---
    if not df_mapping.empty and not df_final.empty:
        try:
            # 1. Clean the join key columns
            df_mapping['BANK ID OF REQUESTER'] = df_mapping['BANK ID OF REQUESTER'].astype(str).str.strip()
            df_final['Bank ID of requestor'] = df_final['Bank ID of requestor'].astype(str).str.strip()
            
            # 2. Create the Roll Up map
            rollup_map = df_mapping.drop_duplicates(subset=['BANK ID OF REQUESTER'], keep='first').set_index('BANK ID OF REQUESTER')['ROLL UP NAME'].to_dict()
            
            # 3. Map the data to the new column
            df_final['ROLL UP'] = df_final['Bank ID of requestor'].map(rollup_map)
            print("✅ Joined 'ROLL UP' data based on 'Bank ID of requestor'.")

        except KeyError as e:
            print(f"⚠️ ERROR: Could not map ROLL UP. Missing required column {e} in mapping file. Setting 'ROLL UP' to None.")
            df_final['ROLL UP'] = None
    else:
        df_final['ROLL UP'] = None


    # --- Step 6: Join from the 'ProjectID_clarity' sheet based on Project ID ---
    if not df_project_clarity.empty and not df_final.empty:
        df_project_clarity['Project ID'] = df_project_clarity['Project ID'].astype(str).str.strip()
        project_clarity_map = df_project_clarity.set_index('Project ID')

        qpr_col = next((col for col in df_project_clarity.columns if 'QPR' in col.upper()), None)
        lifecycle_col = 'Lifecycle status' 
        
        if not qpr_col:
            df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = None, None, None
        else:
            def lookup_and_join(project_ids_str):
                if pd.isna(project_ids_str) or not isinstance(project_ids_str, str): return None, None, None
                ids_list = [id.strip() for id in project_ids_str.split(';')]
                project_names, lifecycle_statuses, qpr_values = [], [], []
                for pid in ids_list:
                    if pid in project_clarity_map.index:
                        row = project_clarity_map.loc[pid]
                        project_names.append(row['Project Name']); lifecycle_statuses.append(row[lifecycle_col]); qpr_values.append(row[qpr_col])
                return (";".join(pd.Series(project_names).dropna().unique()) if project_names else None,
                        ";".join(pd.Series(lifecycle_statuses).dropna().unique()) if lifecycle_statuses else None,
                        ";".join(pd.Series(qpr_values).dropna().unique()) if qpr_values else None)
            df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = df_final['Project_ID'].progress_apply(
                lambda x: pd.Series(lookup_and_join(x)))
            print("✅ Joined 'ProjectID_clarity' data.")
    else:
        df_final[['Project Name', 'Lifecycle Status', 'Quarterly Performance Review(QPR)']] = None, None, None

    # --- Step 7: Final Column Selection and Ordering and Sorting ---
    if not df_final.empty:
        selected_columns = [
            "Bank ID of requestor", "Business Function", "MT Domain", "MT-1 Domain", "Cost Category", "Business Function L6",
            "Position Code", "Position Title", "Country", "Location (Location Name)", "Cost",
            "Employment Type", "Global Grade", "Comments", "Reason for Hire", "People Leader Position", 
            "CW_Number", "Bank_ID", "Project_ID", "Project Name", "Lifecycle Status", "Quarterly Performance Review(QPR)",
            "Financial outlook",
            "ROLL UP" # ADDED new column
        ]
        
        final_cols_present = [col for col in selected_columns if col in df_final.columns]

        df_final = df_final[final_cols_present]

        df_final.sort_values(
            by=['MT Domain', 'MT-1 Domain', 'Cost Category', 'Financial outlook'], 
            inplace=True, 
            na_position='last'
        )
        print("✅ Final file sorted by MT Domain, MT-1 Domain, Cost Category, and Financial outlook.")

        # --- Step 8: Save to new Excel, avoiding overwriting ---
        base_filename = final_output_base_filename
        counter = 1
        output_filename = os.path.join(OUTPUT_FOLDER, base_filename)

        while os.path.exists(output_filename):
            name, ext = os.path.splitext(final_output_base_filename)
            new_filename = f"{name}_{counter}{ext}"
            output_filename = os.path.join(OUTPUT_FOLDER, new_filename)
            counter += 1

        df_final.to_excel(output_filename, index=False)
        print(f"\n🎉 All processing complete. The final report has been saved to '{output_filename}'.")
    else:
        print("Cannot complete processing as the main DataFrame is empty.")
else:
    print("DataFrame is empty after initial loading or filtering. Cannot proceed with further steps.")
